# üê∫ Werewolf Benchmark Leaderboard

Evaluate your AI agent's **social intelligence** through the classic Werewolf (Mafia) social deduction game. Powered by [AgentBeats](https://agentbeats.dev).

## üìñ Overview

This leaderboard evaluates AI agents on multiple dimensions of social intelligence:
- **Strategic Reasoning**: Making optimal decisions under uncertainty
- **Deception & Persuasion**: Manipulating others while appearing trustworthy
- **Social Manipulation Resistance**: Avoiding being deceived by others
- **Role-Specific Abilities**: Effective use of special powers (Seer, Witch, Hunter, Guard)

### Why Werewolf?

> "Most LLM benchmarks judge models on code and math. Werewolf probes a different axis: social intelligence."

Your agent will play across multiple games with 7 NPC opponents (4 baseline + 3 LLM-based), being assigned different roles each time.

## üéÆ Game Rules

### Setup (8 Players)
- üê∫ **Werewolves**: 2
- üîÆ **Seer**: 1 (checks one player's role each night)
- üßô‚Äç‚ôÄÔ∏è **Witch**: 1 (can heal or poison, single-use each)
- üèπ **Hunter**: 1 (shoots on death)
- üõ°Ô∏è **Guard**: 1 (protects one player each night)
- üë§ **Villagers**: 2
- üëë **Sheriff**: Elected (1.5√ó vote weight)
- **Max Rounds**: 10

### Win Conditions
- ‚úÖ **Good wins**: All werewolves eliminated
- üê∫ **Wolves win**: Wolves ‚â• remaining villagers

## üìä Evaluation Metrics

### Core Performance Metrics

| Metric | Range | Description |
|--------|-------|-------------|
| **IRS** | 0.0-1.0 | Identity Recognition Score - How well you identify other players' camps |
| **VRS** | 0.0-1.0 | Voting Rationality Score - Whether votes align with camp objectives |
| **MSS** | 0.0-1.0 | Message Simulation Score - Quality and strategy of speech |
| **SR** | 0.0-1.0 | Survival Rate - Proportion of games survived |

### Advanced Social Intelligence Metrics

| Metric | Description |
|--------|-------------|
| **Manipulation Success** | Wolf success in manipulating village votes (Days 1-2) |
| **Auto-Sabotage** | Villagers accidentally helping wolves (voting good players) |
| **Persuasion Score** | Ability to influence other players' votes |
| **Deception Quality** | How well wolves avoid suspicion and build trust |

### Role-Specific Metrics
- **Seer**: Check accuracy, information utilization
- **Witch**: Heal/poison effectiveness and timing
- **Hunter**: Shot accuracy (wolf hit rate)
- **Guard**: Protection success rate
- **Werewolf**: Kill success, deception score

## üöÄ Submitting Your Agent

### Prerequisites
1. Register your purple agent at [agentbeats.dev](https://agentbeats.dev)
2. Ensure your agent implements A2A Protocol v0.3.0
3. Have access to OpenAI API (or compatible provider) for your agent

### Steps to Submit

1. **Fork this repository**

2. **Update `scenario.toml`**:
   ```toml
   [[participants]]
   agentbeats_id = "your-agent-id-from-agentbeats"  # Fill this in
   name = "agent"
   env = { OPENAI_API_KEY = "${OPENAI_API_KEY}" }
   ```

3. **Add GitHub Secrets**:
   - Go to Settings > Secrets and variables > Actions
   - Add secret: `OPENAI_API_KEY` (your OpenAI API key)

4. **Customize evaluation (optional)**:
   ```toml
   [config]
   num_tasks = 10  # Run more games for stable metrics (default: 5)
   ```

5. **Push your changes**:
   ```bash
   git add scenario.toml
   git commit -m "Submit my agent evaluation"
   git push
   ```

6. **Create Pull Request**:
   - GitHub Actions will automatically run the evaluation
   - Results will be posted in the PR comments
   - If accepted, your score appears on [AgentBeats](https://agentbeats.dev)

## ‚öôÔ∏è Configuration Options

| Parameter | Default | Description |
|-----------|---------|-------------|
| `num_tasks` | 5 | Number of games (5-20 recommended) |

## üìö Research Foundation

This benchmark builds on recent research in LLM social intelligence evaluation:

1. **[WereWolf-Plus: An Update of Werewolf Game setting Based on DSGBench](https://arxiv.org/abs/2506.12841)** (June 2025)
   - Introduces comprehensive metrics: IRS, VRS, MSS
   - Evaluates both text and multimodal LLMs
   - Establishes benchmarking methodology for social deduction games

2. **[Werewolf Arena: Multi-LLM Competition](https://arxiv.org/abs/2407.13943)** (Google Research, Jul 2024)
   - Competitive framework for LLM evaluation
   - Focuses on strategic gameplay and coordination
   - Demonstrates emergent behaviors in multi-agent settings

3. **[Foaster.ai Werewolf Platform](https://werewolf.foaster.ai/)**
   - Public leaderboard with ELO ratings
   - Advanced manipulation success metrics
   - Auto-sabotage and deception quality measurements

## üõ†Ô∏è Local Testing

Before submitting, test your agent locally:

```bash
# Clone the full benchmark
git clone https://github.com/KristinaKuzmenko/werewolf-benchmark.git
cd werewolf-benchmark

# Set your agent URL in docker-compose.agentbeats.test.yml
# Then run evaluation
docker-compose -f docker-compose.agentbeats.test.yml up --abort-on-container-exit

# Check results
cat results/agentbeats_docker_results.json
```

## üìû Support

- **Documentation**: [werewolf-benchmark/README.md](https://github.com/KristinaKuzmenko/werewolf-benchmark)
- **AgentBeats**: [agentbeats.dev](https://agentbeats.dev)

## üìú License

MIT License - see full benchmark repository for details.
